{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f327685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Cài đặt + import\n",
    "!python -m pip install -q chromadb sentence-transformers pypdf python-docx\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "from chromadb import PersistentClient\n",
    "from chromadb.config import Settings\n",
    "from docx import Document\n",
    "from pypdf import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def load_text_with_pages(file_path: Path) -> List[Tuple[int, str]]:\n",
    "    suffix = file_path.suffix.lower()\n",
    "    if suffix == \".pdf\":\n",
    "        reader = PdfReader(str(file_path))\n",
    "        return [(i + 1, (page.extract_text() or \"\")) for i, page in enumerate(reader.pages)]\n",
    "    if suffix in {\".docx\", \".doc\"}:\n",
    "        doc = Document(str(file_path))\n",
    "        paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]\n",
    "        return [(1, \"\\n\".join(paragraphs))]\n",
    "    if suffix == \".txt\":\n",
    "        text = file_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        return [(1, text)]\n",
    "    raise ValueError(f\"Unsupported file type: {suffix}\")\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
    "    text = \" \".join(text.split())\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks = []\n",
    "    step = max(1, chunk_size - overlap)\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        chunk = text[start : start + chunk_size]\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "        start += step\n",
    "    return chunks\n",
    "\n",
    "def build_ids(base: str, page: int, count: int) -> List[str]:\n",
    "    safe_base = base.replace(\" \", \"_\")\n",
    "    return [f\"{safe_base}_p{page}_c{i:04d}\" for i in range(count)]\n",
    "\n",
    "def safe_upsert(collection, ids, documents, metadatas, embeddings):\n",
    "    if hasattr(collection, \"upsert\"):\n",
    "        collection.upsert(ids=ids, documents=documents, metadatas=metadatas, embeddings=embeddings)\n",
    "        return\n",
    "    try:\n",
    "        collection.add(ids=ids, documents=documents, metadatas=metadatas, embeddings=embeddings)\n",
    "    except Exception:\n",
    "        try:\n",
    "            collection.delete(ids=ids)\n",
    "        except Exception:\n",
    "            pass\n",
    "        collection.add(ids=ids, documents=documents, metadatas=metadatas, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c194431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Chọn tài liệu từ data/\n",
    "DOC_PATH = Path(\"data/your_file.pdf\")  # chỉnh lại file cần dùng\n",
    "MODEL_DIR = Path(\"./all-MiniLM-L6-v2\")\n",
    "DB_DIR = Path(\"./chroma_db\")\n",
    "COLLECTION_NAME = \"my_docs\"\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 150\n",
    "TOP_K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6323a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Đọc tài liệu + preview\n",
    "assert DOC_PATH.exists(), f\"File không tồn tại: {DOC_PATH}\"\n",
    "pages = load_text_with_pages(DOC_PATH)\n",
    "print(f\"Pages read: {len(pages)}\")\n",
    "preview_segments = []\n",
    "for _, text in pages:\n",
    "    for seg in text.splitlines():\n",
    "        if seg.strip():\n",
    "            preview_segments.append(seg.strip())\n",
    "        if len(preview_segments) >= 2:\n",
    "            break\n",
    "    if len(preview_segments) >= 2:\n",
    "        break\n",
    "print(\"Preview:\\n- \" + \"\\n- \".join(preview_segments[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b06bd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Chunking\n",
    "all_chunks = []\n",
    "metas = []\n",
    "for page_num, text in pages:\n",
    "    chunks = chunk_text(text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "    ids = build_ids(DOC_PATH.stem, page_num, len(chunks))\n",
    "    all_chunks.extend(chunks)\n",
    "    metas.extend([{'source': str(DOC_PATH), 'page': page_num}] * len(chunks))\n",
    "print(f\"Total chunks: {len(all_chunks)}\")\n",
    "for i, c in list(enumerate(all_chunks[:2])):\n",
    "    print(f\"Chunk {i}: {c[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Load model local + embedding\n",
    "model = SentenceTransformer(str(MODEL_DIR))\n",
    "embeddings = model.encode(all_chunks, normalize_embeddings=True)\n",
    "print(\"Embeddings shape:\", getattr(embeddings, 'shape', (len(embeddings), len(embeddings[0]) if embeddings else 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea93dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Lưu ChromaDB (persist)\n",
    "DB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "client = PersistentClient(path=str(DB_DIR), settings=Settings())\n",
    "collection = client.get_or_create_collection(name=COLLECTION_NAME)\n",
    "ids = [f\"{DOC_PATH.stem}_p{m['page']}_c{i:04d}\" for i, m in enumerate(metas)]\n",
    "safe_upsert(collection, ids, all_chunks, metas, embeddings.tolist())\n",
    "print(\"Collection size:\", collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ec4ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Query thử 3 câu hỏi mẫu\n",
    "queries = [\n",
    "    \"Tóm tắt tài liệu\",\n",
    "    \"Các ý chính là gì?\",\n",
    "    \"Chi tiết quan trọng trong văn bản\",\n",
    "]\n",
    "for q in queries:\n",
    "    q_emb = model.encode([q], normalize_embeddings=True).tolist()[0]\n",
    "    res = collection.query(\n",
    "        query_embeddings=[q_emb],\n",
    "        n_results=TOP_K,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"],\n",
    "    )\n",
    "    docs = res.get(\"documents\", [[]])[0]\n",
    "    metas_res = res.get(\"metadatas\", [[]])[0]\n",
    "    dists = res.get(\"distances\", [[]])[0]\n",
    "    print(f\"\n",
    "Query: {q}\")\n",
    "    for rank, (doc, meta, dist) in enumerate(zip(docs, metas_res, dists), start=1):\n",
    "        source = meta.get('source', '') if isinstance(meta, dict) else ''\n",
    "        page = meta.get('page', '?') if isinstance(meta, dict) else '?'\n",
    "        preview = (doc or '')[:400].replace('\\n', ' ')\n",
    "        print(f\"#{rank} | dist={dist:.4f} | source={source} | page={page}\n",
    "  {preview}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
